# -*- coding: utf-8 -*-
"""fineTuningHW1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UiDP3XowxoZb4aoHcDwA-W6DgyOTZzUO
"""

import os
os.environ["OPENAI_API_KEY"] = "Your-API-KEY"
OPENAI_API_KEY=os.environ['OPENAI_API_KEY' ]

import json

# Input and output file paths
input_file_path = "data_prepared.jsonl"
output_file_path = "transformed_data.jsonl"

# System message to be added
system_message = {"role": "system", "content": "You are a helpful assistant."}

# Read and transform the data
with open(input_file_path, 'r') as infile, open(output_file_path, 'w') as outfile:
    for line in infile:
        # Skip empty lines
        if not line.strip():
            continue
        try:
            # Load each JSON object
            data = json.loads(line.strip())

            # Create the new format
            transformed_data = {
                "messages": [
                    system_message,
                    {"role": "user", "content": data['prompt']},
                    {"role": "assistant", "content": data['completion']}
                ]
            }

            # Write the transformed data to the output file
            json.dump(transformed_data, outfile)
            outfile.write('\n')
        except json.JSONDecodeError as e:
            print(f"Skipping invalid JSON line: {line.strip()} (Error: {e})")

print("Transformation complete. Output saved to", output_file_path)



from openai import OpenAI

client = OpenAI()

client.files.create(file=open("transformed_data.jsonl","rb"),purpose='fine-tune')

file_id='file-c1VBooDjoh1qOBTaB1QXoJUu'

files = client.files.list()

for file in files.data:
  print(file)

for file in files.data:
  if file.filename == 'transformed_data.jsonl':
    training_file_id = file.id

client.fine_tuning.jobs.create(
    training_file=training_file_id,
    model="gpt-3.5-turbo"
)

jobs=client.fine_tuning.jobs.list()

client.fine_tuning.jobs.retrieve('ftjob-aA4Fovxi16bgSAujl2LMukld')

import openai

# Replace with your fine-tuned model ID
fine_tuned_model = 'ft:gpt-3.5-turbo-0125:personal::AVpoP52J'

# Questions from the dataset
questions = [
    "How often should I water the indoor plants?",
    "Where are the extra batteries stored?",
    "Can I reset the Wi-Fi router on my own?",
    "What is the best way to clean the coffee machine?",
    "When should I take the trash out?",
    "Where is the toolbox located?",
    "How do I set the thermostat to eco mode?",
    "What should I do if the smoke detector beeps?",
    "Where is the emergency flashlight?"
]

# Loop through each question to get responses from the fine-tuned model
for question in questions:
    prompt = f"Q: {question}\nA:"
    print("Prompt:", prompt)

    # Send the request to the fine-tuned model
    response = client.chat.completions.create(
        model=fine_tuned_model,
        messages=[
            {"role": "system", "content": "You are a helpful assistant providing accurate answers to household-related queries."},
            {"role": "user", "content": prompt},
        ],
    )

    # Extract and print the completion (response structure update)
    answer = response.choices[0].message.content.strip() # This may need to be updated depending on the response format
    print(f"Q: {question}")
    print(f"A: {answer}")
    print("-" * 50)